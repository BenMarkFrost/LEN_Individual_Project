{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Testing\n",
    "\n",
    "Benjamin Frost 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from torch_explain.models.explainer import Explainer\n",
    "from torch_explain.logic.metrics import formula_consistency\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from torch.nn.functional import one_hot\n",
    "from func_timeout import func_set_timeout, func_timeout, FunctionTimedOut\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "seed_everything(42)\n",
    "base_dir = f'./runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breastCancer.csv', 'clusteredData.csv', 'clusteredDataSepsis.csv', 'expertLabelledData.csv', 'metricExtractedData.csv', 'staticData.csv']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"./categorisedData/\")\n",
    "\n",
    "datasets = {file : pd.read_csv(\"./categorisedData/\" + file) for file in files}\n",
    "\n",
    "print(files)\n",
    "\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the timeout wrapper around the explainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_set_timeout(90)\n",
    "def explain_with_timeout(model, val_data, train_data, test_data, topk_expl, concepts):\n",
    "\n",
    "    return model.explain_class(val_dataloaders=val_data, train_dataloaders=train_data, test_dataloaders=test_data, topk_explanations=topk_expl, concept_names=concepts, max_minterm_complexity=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes in each hidden layer, learning rate\n",
    "\n",
    "hiddenLayers = {\n",
    "    'breastCancer.csv' : [[20], 0.01],\n",
    "    'clusteredData.csv' : [[20], 0.01], \n",
    "    'clusteredDataSepsis.csv' : [[20, 40, 20], 0.0001],\n",
    "    'expertLabelledData.csv' : [[20], 0.01],\n",
    "    'metricExtractedData.csv' : [[20, 20], 0.01],\n",
    "    'staticData.csv': [[20], 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training breastCancer.csv\n",
      "\n",
      "0    458\n",
      "1    241\n",
      "Name: Mortality14Days, dtype: int64\n",
      "There are 89 concepts\n",
      "Split [1/5]\n",
      "447/112/140\n",
      "[298 149]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[298 298]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.311\n",
      "Metric val_loss improved by 0.083 >= min_delta = 0.0. New best score: 0.228\n",
      "Metric val_loss improved by 0.060 >= min_delta = 0.0. New best score: 0.168\n",
      "Metric val_loss improved by 0.020 >= min_delta = 0.0. New best score: 0.149\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.132\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.126\n",
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.113\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.113. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.9519560741249142, 0.947463768115942, 0.9569842738205365]\n",
      "breastCancer.csv split 1 scores: [0.9519560741249142, 0.947463768115942, 0.9569842738205365]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch          0.976102352142334\n",
      "     test_acc_epoch         0.9785714149475098\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.049743413925171\n",
      "Number of features that impact on target 0: 9\n",
      "Explanation for target 0: Bare_Nuclei_1 & ~Cell_Shape_Uniformity_10\n",
      "Explanation accuracy: 0.9001700400416872\n",
      "Number of features that impact on target 1: 6\n",
      "Explanation for target 1: ~Bare_Nuclei_1\n",
      "Explanation accuracy: 0.8755555555555556\n",
      "Split [2/5]\n",
      "447/112/140\n",
      "[292 155]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292 292]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.298\n",
      "Metric val_loss improved by 0.104 >= min_delta = 0.0. New best score: 0.194\n",
      "Metric val_loss improved by 0.077 >= min_delta = 0.0. New best score: 0.117\n",
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 0.096\n",
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.081\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.064\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.064. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.945793462027767, 0.9569746376811594, 0.9376490999783127]\n",
      "breastCancer.csv split 2 scores: [0.945793462027767, 0.9569746376811594, 0.9376490999783127]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9533333778381348\n",
      "     test_acc_epoch         0.9571428298950195\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 2.6255316734313965\n",
      "Number of features that impact on target 0: 7\n",
      "Explanation for target 0: Single_Epi_Cell_Size_2\n",
      "Explanation accuracy: 0.8134228903459673\n",
      "Number of features that impact on target 1: 4\n",
      "Explanation for target 1: (~Cell_Shape_Uniformity_1 & ~Normal_Nucleoli_2) | (~Cell_Shape_Uniformity_1 & ~Cell_Shape_Uniformity_2 & ~Bare_Nuclei_1)\n",
      "Explanation accuracy: 0.7973531844499586\n",
      "Split [3/5]\n",
      "447/112/140\n",
      "[302 145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302 302]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.055 >= min_delta = 0.0. New best score: 0.184\n",
      "Metric val_loss improved by 0.094 >= min_delta = 0.0. New best score: 0.090\n",
      "Metric val_loss improved by 0.039 >= min_delta = 0.0. New best score: 0.051\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.050\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.048\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.037\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.036\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.031\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.030\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.030. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.9765611920308053, 0.9836956521739131, 0.9705882352941176]\n",
      "breastCancer.csv split 3 scores: [0.9765611920308053, 0.9836956521739131, 0.9705882352941176]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9688888788223267\n",
      "     test_acc_epoch         0.9714285731315613\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 2.1465632915496826\n",
      "Number of features that impact on target 0: 6\n",
      "Explanation for target 0: Cell_Shape_Uniformity_1\n",
      "Explanation accuracy: 0.8458392323422999\n",
      "Number of features that impact on target 1: 2\n",
      "Explanation for target 1: ~Clump_Thickness_2 & ~Cell_Shape_Uniformity_1\n",
      "Explanation accuracy: 0.8372437116888608\n",
      "Split [4/5]\n",
      "447/112/140\n",
      "[293 154]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293 293]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.236\n",
      "Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 0.206\n",
      "Metric val_loss improved by 0.040 >= min_delta = 0.0. New best score: 0.165\n",
      "Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.112\n",
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.098\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.096\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.093\n",
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.089\n",
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.082\n",
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.078\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.073\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.073. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.9519560741249142, 0.9434850863422293, 0.9625730994152046]\n",
      "breastCancer.csv split 4 scores: [0.9519560741249142, 0.9434850863422293, 0.9625730994152046]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9442389607429504\n",
      "     test_acc_epoch          0.949999988079071\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.7234373092651367\n",
      "Number of features that impact on target 0: 5\n",
      "Explanation for target 0: (Cell_Size_Uniformity_1 & ~Normal_Nucleoli_10) | (~Clump_Thickness_9 & ~Bare_Nuclei_10 & ~Bland_Chromatin_10 & ~Normal_Nucleoli_10)\n",
      "Explanation accuracy: 0.8715674882270198\n",
      "Number of features that impact on target 1: 5\n",
      "Explanation for target 1: ~Cell_Shape_Uniformity_1 & ~Normal_Nucleoli_2\n",
      "Explanation accuracy: 0.8363095238095238\n",
      "Split [5/5]\n",
      "448/112/139\n",
      "[296 152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296 296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.097 >= min_delta = 0.0. New best score: 0.172\n",
      "Metric val_loss improved by 0.029 >= min_delta = 0.0. New best score: 0.142\n",
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.128\n",
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.102\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.099\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.089\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.087\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.084\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.084. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.9440322116767329, 0.9418498168498168, 0.9463459759481961]\n",
      "breastCancer.csv split 5 scores: [0.9440322116767329, 0.9418498168498168, 0.9463459759481961]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9363553524017334\n",
      "     test_acc_epoch         0.9424460530281067\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.4893903732299805\n",
      "Number of features that impact on target 0: 4\n",
      "Explanation for target 0: Cell_Shape_Uniformity_1 | (Single_Epi_Cell_Size_2 & ~Bare_Nuclei_10)\n",
      "Explanation accuracy: 0.9120506183491517\n",
      "Number of features that impact on target 1: 2\n",
      "Explanation for target 1: ~Cell_Size_Uniformity_1 & ~Cell_Shape_Uniformity_2 & ~Bare_Nuclei_1\n",
      "Explanation accuracy: 0.9350163627863488\n",
      "Training clusteredData.csv\n",
      "\n",
      "0    924\n",
      "1     35\n",
      "Name: Mortality14Days, dtype: int64\n",
      "There are 48 concepts\n",
      "Split [1/5]\n",
      "613/154/192\n",
      "[592  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.0 K \n",
      "-------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592 592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.419\n",
      "Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.362\n",
      "Metric val_loss improved by 0.053 >= min_delta = 0.0. New best score: 0.309\n",
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.277\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.275\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.272\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.268\n",
      "Metric val_loss improved by 0.033 >= min_delta = 0.0. New best score: 0.235\n",
      "Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.213\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.213. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.488, 0.4945945945945946, 0.48157894736842105]\n",
      "clusteredData.csv split 1 scores: [0.488, 0.4945945945945946, 0.48157894736842105]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4880000054836273\n",
      "     test_acc_epoch              0.953125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.8098030090332031\n",
      "Number of features that impact on target 0: 10\n",
      "Explanation for target 0: Arterial BP [Systolic]_StdDev_low\n",
      "Explanation accuracy: 0.439845720219552\n",
      "Number of features that impact on target 1: 4\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [2/5]\n",
      "613/154/192\n",
      "[591  22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.0 K \n",
      "-------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[591 591]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.489\n",
      "Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.453\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.444\n",
      "Metric val_loss improved by 0.082 >= min_delta = 0.0. New best score: 0.361\n",
      "Metric val_loss improved by 0.077 >= min_delta = 0.0. New best score: 0.285\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.285. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.47540983606557374, 0.4702702702702703, 0.48066298342541436]\n",
      "clusteredData.csv split 2 scores: [0.47540983606557374, 0.4702702702702703, 0.48066298342541436]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4893616735935211\n",
      "     test_acc_epoch         0.9583333134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.9948132038116455\n",
      "Number of features that impact on target 0: 9\n",
      "Explanation for target 0: SVRI_StdDev_low | (~Hemoglobin_StdDev_high & ~SVRI_Mean_high)\n",
      "Explanation accuracy: 0.4886363636363637\n",
      "Number of features that impact on target 1: 8\n",
      "Explanation for target 1: ~Arterial BP [Systolic]_Mean_high & ~Arterial pH_StdDev_low & ~Hemoglobin_StdDev_low & ~SVR_StdDev_high\n",
      "Explanation accuracy: 0.5252747252747253\n",
      "Split [3/5]\n",
      "613/154/192\n",
      "[591  22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.0 K \n",
      "-------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[591 591]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.423\n",
      "Metric val_loss improved by 0.098 >= min_delta = 0.0. New best score: 0.325\n",
      "Metric val_loss improved by 0.033 >= min_delta = 0.0. New best score: 0.292\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.289\n",
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.279\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.279. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.47107438016528924, 0.46216216216216216, 0.4803370786516854]\n",
      "clusteredData.csv split 3 scores: [0.47107438016528924, 0.46216216216216216, 0.4803370786516854]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4880000054836273\n",
      "     test_acc_epoch              0.953125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.5424087047576904\n",
      "Number of features that impact on target 0: 5\n",
      "Explanation for target 0: Arterial BP [Diastolic]_Mean_low | Arterial PaO2_Mean_high | Hemoglobin_Mean_low\n",
      "Explanation accuracy: 0.49378531073446325\n",
      "Number of features that impact on target 1: 8\n",
      "Explanation for target 1: ~Arterial BP Mean_StdDev_high & ~Arterial BP [Systolic]_Mean_high & ~Arterial PaO2_Mean_high & ~Hemoglobin_StdDev_low & ~SVRI_Mean_high\n",
      "Explanation accuracy: 0.4666666666666667\n",
      "Split [4/5]\n",
      "613/154/192\n",
      "[590  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.0 K \n",
      "-------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[590 590]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.448\n",
      "Metric val_loss improved by 0.072 >= min_delta = 0.0. New best score: 0.376\n",
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.359\n",
      "Metric val_loss improved by 0.114 >= min_delta = 0.0. New best score: 0.246\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.246. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.507966573816156, 0.5254826254826255, 0.5105363984674329]\n",
      "clusteredData.csv split 4 scores: [0.507966573816156, 0.5254826254826255, 0.5105363984674329]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch          0.47826087474823\n",
      "     test_acc_epoch         0.9166666865348816\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.8957555294036865\n",
      "Number of features that impact on target 0: 11\n",
      "Explanation for target 0: Platelets_StdDev_high | ~Arterial PaCO2_StdDev_high | ~Arterial pH_Mean_low\n",
      "Explanation accuracy: 0.4768392370572207\n",
      "Number of features that impact on target 1: 1\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [5/5]\n",
      "614/154/191\n",
      "[589  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.0 K \n",
      "-------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589 589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.534\n",
      "Metric val_loss improved by 0.053 >= min_delta = 0.0. New best score: 0.481\n",
      "Metric val_loss improved by 0.091 >= min_delta = 0.0. New best score: 0.391\n",
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.376\n",
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.364\n",
      "Metric val_loss improved by 0.043 >= min_delta = 0.0. New best score: 0.321\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.320\n",
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.294\n",
      "Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.237\n",
      "Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.210\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.207\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.207. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.4104938271604938, 0.36141304347826086, 0.475]\n",
      "clusteredData.csv split 5 scores: [0.4104938271604938, 0.36141304347826086, 0.475]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch          0.485175222158432\n",
      "     test_acc_epoch         0.9424083828926086\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.5616412162780762\n",
      "Number of features that impact on target 0: 10\n",
      "Explanation for target 0: Arterial PaO2_StdDev_high | SVR_Mean_low\n",
      "Explanation accuracy: 0.45838198498748955\n",
      "Number of features that impact on target 1: 2\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Training clusteredDataSepsis.csv\n",
      "\n",
      "0    31606\n",
      "1     2422\n",
      "Name: Mortality14Days, dtype: int64\n",
      "There are 72 concepts\n",
      "Split [1/5]\n",
      "21777/5445/6806\n",
      "[20225  1552]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 4.6 K \n",
      "-------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20225 20225]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.634\n",
      "Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.580\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.569\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.558\n",
      "Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.504\n",
      "Metric val_loss improved by 0.039 >= min_delta = 0.0. New best score: 0.465\n",
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.457\n",
      "Metric val_loss improved by 0.035 >= min_delta = 0.0. New best score: 0.422\n",
      "Metric val_loss improved by 0.044 >= min_delta = 0.0. New best score: 0.379\n",
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.364\n",
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.361\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.359\n",
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.348\n",
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.344\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.344. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5559768464458875, 0.5651254572122537, 0.5505680063480898]\n",
      "clusteredDataSepsis.csv split 1 scores: [0.5559768464458875, 0.5651254572122537, 0.5505680063480898]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5646570324897766\n",
      "     test_acc_epoch         0.8729062080383301\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 9.052464008331299\n",
      "Number of features that impact on target 0: 45\n",
      "Explanation for target 0: SBP_StdDev_high | Gender_high | Gender_low\n",
      "Explanation accuracy: 0.4815661182205972\n",
      "Number of features that impact on target 1: 46\n",
      "Explanation for target 1: Gender_high & ~Age_low\n",
      "Explanation accuracy: 0.44265345424282715\n",
      "Split [2/5]\n",
      "21777/5445/6806\n",
      "[20213  1564]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 4.6 K \n",
      "-------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20213 20213]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.638\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.634\n",
      "Metric val_loss improved by 0.059 >= min_delta = 0.0. New best score: 0.576\n",
      "Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.538\n",
      "Metric val_loss improved by 0.059 >= min_delta = 0.0. New best score: 0.479\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.473\n",
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.447\n",
      "Metric val_loss improved by 0.020 >= min_delta = 0.0. New best score: 0.427\n",
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.413\n",
      "Metric val_loss improved by 0.028 >= min_delta = 0.0. New best score: 0.386\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.383\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.382\n",
      "Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.365\n",
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 0.344\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.344. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5649893957957862, 0.5816424388024211, 0.5569792981363406]\n",
      "clusteredDataSepsis.csv split 2 scores: [0.5649893957957862, 0.5816424388024211, 0.5569792981363406]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5679110288619995\n",
      "     test_acc_epoch         0.8853952288627625\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 11.816979169845581\n",
      "Number of features that impact on target 0: 50\n",
      "Explanation for target 0: Platelets_Mean_low | ~Potassium_Mean_low | ~Gender_high\n",
      "Explanation accuracy: 0.48152662451435974\n",
      "Number of features that impact on target 1: 49\n",
      "Explanation for target 1: ~SBP_StdDev_high\n",
      "Explanation accuracy: 0.3630895208932751\n",
      "Split [3/5]\n",
      "21777/5445/6806\n",
      "[20264  1513]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 4.6 K \n",
      "-------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20264 20264]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.649\n",
      "Metric val_loss improved by 0.052 >= min_delta = 0.0. New best score: 0.597\n",
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 0.576\n",
      "Metric val_loss improved by 0.052 >= min_delta = 0.0. New best score: 0.525\n",
      "Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.487\n",
      "Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.450\n",
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.437\n",
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.425\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.422\n",
      "Metric val_loss improved by 0.023 >= min_delta = 0.0. New best score: 0.399\n",
      "Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.357\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.357. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5487538468328955, 0.5678631366236256, 0.542745331538435]\n",
      "clusteredDataSepsis.csv split 3 scores: [0.5487538468328955, 0.5678631366236256, 0.542745331538435]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5627663135528564\n",
      "     test_acc_epoch         0.8817219138145447\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 12.240967512130737\n",
      "Number of features that impact on target 0: 52\n",
      "Explanation for target 0: Gender_high | ~Gender_high\n",
      "Explanation accuracy: 0.48152662451435974\n",
      "Number of features that impact on target 1: 48\n",
      "Explanation for target 1: ~Age_low\n",
      "Explanation accuracy: 0.31979061718714274\n",
      "Split [4/5]\n",
      "21778/5445/6805\n",
      "[20224  1554]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 4.6 K \n",
      "-------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20224 20224]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.663\n",
      "Metric val_loss improved by 0.073 >= min_delta = 0.0. New best score: 0.590\n",
      "Metric val_loss improved by 0.028 >= min_delta = 0.0. New best score: 0.562\n",
      "Metric val_loss improved by 0.025 >= min_delta = 0.0. New best score: 0.537\n",
      "Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.491\n",
      "Metric val_loss improved by 0.059 >= min_delta = 0.0. New best score: 0.432\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.426\n",
      "Metric val_loss improved by 0.062 >= min_delta = 0.0. New best score: 0.364\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.358\n",
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.346\n",
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.342\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.341\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.341. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5748453123808115, 0.585928643992673, 0.5676723374547971]\n",
      "clusteredDataSepsis.csv split 4 scores: [0.5748453123808115, 0.585928643992673, 0.5676723374547971]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5806083679199219\n",
      "     test_acc_epoch         0.8921381235122681\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 7.75366997718811\n",
      "Number of features that impact on target 0: 55\n",
      "Explanation for target 0: Gender_low | (Glucose_StdDev_low & Gender_high)\n",
      "Explanation accuracy: 0.4815633094621362\n",
      "Number of features that impact on target 1: 51\n",
      "Explanation for target 1: ~Age_high & ~Age_low\n",
      "Explanation accuracy: 0.4815633094621362\n",
      "Split [5/5]\n",
      "21778/5445/6805\n",
      "[20242  1536]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 4.6 K \n",
      "-------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20242 20242]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.632\n",
      "Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.574\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.573\n",
      "Metric val_loss improved by 0.071 >= min_delta = 0.0. New best score: 0.502\n",
      "Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.464\n",
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.440\n",
      "Metric val_loss improved by 0.044 >= min_delta = 0.0. New best score: 0.395\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.389\n",
      "Metric val_loss improved by 0.025 >= min_delta = 0.0. New best score: 0.365\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.348\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.339\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.339. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5628175979216188, 0.5816017642882638, 0.5547182503100305]\n",
      "clusteredDataSepsis.csv split 5 scores: [0.5628175979216188, 0.5816017642882638, 0.5547182503100305]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5678743720054626\n",
      "     test_acc_epoch         0.8803820610046387\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 13.367179870605469\n",
      "Number of features that impact on target 0: 57\n",
      "Explanation for target 0: Unit1_low & ~ICULOS_high\n",
      "Explanation accuracy: 0.46630467739648546\n",
      "Number of features that impact on target 1: 43\n",
      "Explanation for target 1: Platelets_Mean_high\n",
      "Explanation accuracy: 0.3847940248809004\n",
      "Training expertLabelledData.csv\n",
      "\n",
      "0    1077\n",
      "1      49\n",
      "Name: Mortality14Days, dtype: int64\n",
      "There are 140 concepts\n",
      "Split [1/5]\n",
      "720/180/226\n",
      "[684  36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 5.7 K \n",
      "-------------------------------------------\n",
      "5.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.7 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[684 684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.435\n",
      "Metric val_loss improved by 0.247 >= min_delta = 0.0. New best score: 0.188\n",
      "Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.152\n",
      "Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.130\n",
      "Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 0.089\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.080\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.080. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5221987315010571, 0.524537037037037, 0.5206386292834891]\n",
      "expertLabelledData.csv split 1 scores: [0.5221987315010571, 0.524537037037037, 0.5206386292834891]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4851936101913452\n",
      "     test_acc_epoch         0.9424778819084167\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 2.543801784515381\n",
      "Number of features that impact on target 0: 6\n",
      "Explanation for target 0: Arterial_BP_Diastolic_medium\n",
      "Explanation accuracy: 0.42346938775510207\n",
      "Number of features that impact on target 1: 7\n",
      "Explanation for target 1: ~Arterial_pH_Min_medium & ~CVP_Min_low & ~Daily_Weight_low & ~SVI_medium\n",
      "Explanation accuracy: 0.4593301435406698\n",
      "Split [2/5]\n",
      "720/181/225\n",
      "[687  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 5.7 K \n",
      "-------------------------------------------\n",
      "5.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.7 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[687 687]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.380\n",
      "Metric val_loss improved by 0.130 >= min_delta = 0.0. New best score: 0.250\n",
      "Metric val_loss improved by 0.028 >= min_delta = 0.0. New best score: 0.221\n",
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.195\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.195. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.6038732394366197, 0.6388888888888888, 0.5857142857142857]\n",
      "expertLabelledData.csv split 2 scores: [0.6038732394366197, 0.6388888888888888, 0.5857142857142857]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.6111751198768616\n",
      "     test_acc_epoch         0.9466667175292969\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 3.1704859733581543\n",
      "Number of features that impact on target 0: 7\n",
      "Explanation for target 0: Hamoglobin_medium | ~SVI_low | ~Sodium_Max_medium\n",
      "Explanation accuracy: 0.5254745254745256\n",
      "Number of features that impact on target 1: 7\n",
      "Explanation for target 1: ~Arterial_pH_Min_medium & ~Heart_Rate_Max_medium & ~NBP_Mean_high\n",
      "Explanation accuracy: 0.3682356544823361\n",
      "Split [3/5]\n",
      "720/181/225\n",
      "[690  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 5.7 K \n",
      "-------------------------------------------\n",
      "5.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.7 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[690 690]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.485\n",
      "Metric val_loss improved by 0.196 >= min_delta = 0.0. New best score: 0.289\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.272\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.267\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.267. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.47183098591549294, 0.46744186046511627, 0.476303317535545]\n",
      "expertLabelledData.csv split 3 scores: [0.47183098591549294, 0.46744186046511627, 0.476303317535545]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.47795823216438293\n",
      "     test_acc_epoch         0.9155555963516235\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 3.6726107597351074\n",
      "Number of features that impact on target 0: 6\n",
      "Explanation for target 0: Arterial_pH_Min_high | ~CVP_Max_low\n",
      "Explanation accuracy: 0.4578313253012048\n",
      "Number of features that impact on target 1: 5\n",
      "Explanation for target 1: (SVI_low & ~CVP_Min_low) | (SVI_low & ~Daily_Weight_low)\n",
      "Explanation accuracy: 0.38773995646150805\n",
      "Split [4/5]\n",
      "720/181/225\n",
      "[690  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 5.7 K \n",
      "-------------------------------------------\n",
      "5.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.7 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[690 690]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.351\n",
      "Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 0.320\n",
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.296\n",
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.264\n",
      "Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.237\n",
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.230\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.224\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.218\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.207\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.207. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.4779582366589327, 0.4790697674418605, 0.47685185185185186]\n",
      "expertLabelledData.csv split 4 scores: [0.4779582366589327, 0.4790697674418605, 0.47685185185185186]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.48394498229026794\n",
      "     test_acc_epoch         0.9377778172492981\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 2.1718990802764893\n",
      "Number of features that impact on target 0: 7\n",
      "Explanation for target 0: Arterial_pH_Min_medium | Sodium_Max_low | (Heart_Rate_Max_medium & ~Hamoglobin_high)\n",
      "Explanation accuracy: 0.45223647396533556\n",
      "Number of features that impact on target 1: 1\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [5/5]\n",
      "720/181/225\n",
      "[689  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 5.7 K \n",
      "-------------------------------------------\n",
      "5.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.7 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[689 689]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.343\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.342\n",
      "Metric val_loss improved by 0.034 >= min_delta = 0.0. New best score: 0.308\n",
      "Metric val_loss improved by 0.019 >= min_delta = 0.0. New best score: 0.289\n",
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.274\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.274. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5576671035386631, 0.5430232558139535, 0.6046380090497738]\n",
      "expertLabelledData.csv split 5 scores: [0.5576671035386631, 0.5430232558139535, 0.6046380090497738]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5917997360229492\n",
      "     test_acc_epoch         0.9111111164093018\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.461946964263916\n",
      "Number of features that impact on target 0: 2\n",
      "Explanation for target 0: Heart_Rate_Max_medium | ~SVI_low\n",
      "Explanation accuracy: 0.4122387727879057\n",
      "Number of features that impact on target 1: 5\n",
      "Explanation for target 1: SVI_low & Sodium_Max_medium & ~CVP_Min_low & ~CaO2_high\n",
      "Explanation accuracy: 0.5507745266781411\n",
      "Training metricExtractedData.csv\n",
      "\n",
      "0    924\n",
      "1     35\n",
      "Name: Mortality14Days, dtype: int64\n",
      "There are 70 concepts\n",
      "Split [1/5]\n",
      "613/154/192\n",
      "[587  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.3 K \n",
      "-------------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[587 587]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.374\n",
      "Metric val_loss improved by 0.063 >= min_delta = 0.0. New best score: 0.312\n",
      "Metric val_loss improved by 0.069 >= min_delta = 0.0. New best score: 0.242\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.231\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.223\n",
      "Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.185\n",
      "Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.142\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.142. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5620982790794111, 0.5579150579150579, 0.5672043010752689]\n",
      "metricExtractedData.csv split 1 scores: [0.5620982790794111, 0.5579150579150579, 0.5672043010752689]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4880000054836273\n",
      "     test_acc_epoch              0.953125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.3118126392364502\n",
      "Number of features that impact on target 0: 6\n",
      "Explanation for target 0: CVP__quantile__q_0.1_medium | CVP__root_mean_square_high | ~CVP__c3__lag_1_high\n",
      "Explanation accuracy: 0.488\n",
      "Number of features that impact on target 1: 6\n",
      "Explanation for target 1: ~CVP__quantile__q_0.1_medium & ~CVP__root_mean_square_high & ~CVP__root_mean_square_low & ~CVP__quantile__q_0.7_medium\n",
      "Explanation accuracy: 0.49378531073446325\n",
      "Split [2/5]\n",
      "613/154/192\n",
      "[591  22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.3 K \n",
      "-------------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[591 591]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.388\n",
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.376\n",
      "Metric val_loss improved by 0.075 >= min_delta = 0.0. New best score: 0.301\n",
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.293\n",
      "Metric val_loss improved by 0.060 >= min_delta = 0.0. New best score: 0.234\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.233\n",
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.217\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.212\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.212. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5919854280510017, 0.6185328185328185, 0.5770969362129583]\n",
      "metricExtractedData.csv split 2 scores: [0.5919854280510017, 0.6185328185328185, 0.5770969362129583]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5620983242988586\n",
      "     test_acc_epoch         0.9427083134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.9273629188537598\n",
      "Number of features that impact on target 0: 1\n",
      "Explanation for target 0: CVP__quantile__q_0.2_high | CVP__quantile__q_0.2_medium | CVP__root_mean_square_high\n",
      "Explanation accuracy: 0.46404100898340356\n",
      "Number of features that impact on target 1: 9\n",
      "Explanation for target 1: ~CVP__quantile__q_0.4_very_low & ~CVP__quantile__q_0.2_high & ~CVP__quantile__q_0.2_low & ~CVP__quantile__q_0.1_very_high & ~CVP__c3__lag_3_very_high\n",
      "Explanation accuracy: 0.30859375000000006\n",
      "Split [3/5]\n",
      "613/154/192\n",
      "[592  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.3 K \n",
      "-------------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592 592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.356\n",
      "Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.314\n",
      "Metric val_loss improved by 0.078 >= min_delta = 0.0. New best score: 0.236\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.236. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.48387096774193544, 0.4864864864864865, 0.48128342245989303]\n",
      "metricExtractedData.csv split 3 scores: [0.48387096774193544, 0.4864864864864865, 0.48128342245989303]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch           0.4810810983181\n",
      "     test_acc_epoch         0.9270833134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.401003122329712\n",
      "Number of features that impact on target 0: 2\n",
      "Explanation for target 0: CVP__minimum_low | CVP__minimum_very_low\n",
      "Explanation accuracy: 0.41646478438972817\n",
      "Number of features that impact on target 1: 2\n",
      "Explanation for target 1: ~CVP__quantile__q_0.4_medium & ~CVP__minimum_very_low & ~CVP__c3__lag_3_medium & ~CVP__quantile__q_0.7_low\n",
      "Explanation accuracy: 0.4037267080745342\n",
      "Split [4/5]\n",
      "613/154/192\n",
      "[589  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.3 K \n",
      "-------------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589 589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.376\n",
      "Metric val_loss improved by 0.081 >= min_delta = 0.0. New best score: 0.296\n",
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.280\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.277\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.271\n",
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.259\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.259. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.514392388293388, 0.5308880308880309, 0.5142045454545454]\n",
      "metricExtractedData.csv split 4 scores: [0.514392388293388, 0.5308880308880309, 0.5142045454545454]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch          0.54347825050354\n",
      "     test_acc_epoch         0.9270833134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.6613006591796875\n",
      "Number of features that impact on target 0: 6\n",
      "Explanation for target 0: CVP__quantile__q_0.4_medium | CVP__root_mean_square_high | CVP__quantile__q_0.7_low\n",
      "Explanation accuracy: 0.45841674249317554\n",
      "Number of features that impact on target 1: 3\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [5/5]\n",
      "614/154/191\n",
      "[589  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.3 K \n",
      "-------------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589 589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.314\n",
      "Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.256\n",
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.232\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.222\n",
      "Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.185\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.167\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.150\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.147\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.147. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5787797108551826, 0.5632763975155279, 0.6089572192513368]\n",
      "metricExtractedData.csv split 5 scores: [0.5787797108551826, 0.5632763975155279, 0.6089572192513368]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4837837815284729\n",
      "     test_acc_epoch         0.9371727705001831\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.835308313369751\n",
      "Number of features that impact on target 0: 7\n",
      "Explanation for target 0: CVP__mean_very_low | (~CVP__c3__lag_2_medium & ~CVP__quantile__q_0.7_low)\n",
      "Explanation accuracy: 0.49094752320558777\n",
      "Number of features that impact on target 1: 5\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Training staticData.csv\n",
      "\n",
      "0    4584\n",
      "1     678\n",
      "Name: Mortality14Days, dtype: int64\n",
      "There are 31 concepts\n",
      "Split [1/5]\n",
      "3367/842/1053\n",
      "[2928  439]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2928 2928]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.523\n",
      "Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 0.481\n",
      "Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.455\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.455. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5418881941360728, 0.5694600359227661, 0.5429151061173533]\n",
      "staticData.csv split 1 scores: [0.5418881941360728, 0.5694600359227661, 0.5429151061173533]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5770694017410278\n",
      "     test_acc_epoch         0.7853751182556152\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.0778207778930664\n",
      "Number of features that impact on target 0: 10\n",
      "Explanation for target 0: (respiration_high & renal_low) | (coagulation_medium & renal_low) | (renal_low & cns_high)\n",
      "Explanation accuracy: 0.29539509702536654\n",
      "Number of features that impact on target 1: 3\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [2/5]\n",
      "3367/842/1053\n",
      "[2946  421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2946 2946]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.759\n",
      "Metric val_loss improved by 0.155 >= min_delta = 0.0. New best score: 0.604\n",
      "Metric val_loss improved by 0.127 >= min_delta = 0.0. New best score: 0.477\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.472\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.472. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.6002813504823151, 0.5910537879273847, 0.6150553213909378]\n",
      "staticData.csv split 2 scores: [0.6002813504823151, 0.5910537879273847, 0.6150553213909378]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5753709673881531\n",
      "     test_acc_epoch         0.8385564684867859\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.1893930435180664\n",
      "Number of features that impact on target 0: 5\n",
      "Explanation for target 0: coagulation_medium | (age_medium & ~renal_medium)\n",
      "Explanation accuracy: 0.42816268966411625\n",
      "Number of features that impact on target 1: 4\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [3/5]\n",
      "3368/842/1052\n",
      "[2935  433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2935 2935]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.668\n",
      "Metric val_loss improved by 0.121 >= min_delta = 0.0. New best score: 0.547\n",
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.531\n",
      "Metric val_loss improved by 0.059 >= min_delta = 0.0. New best score: 0.472\n",
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.458\n",
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.432\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.432. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5544551229841994, 0.5739731006906579, 0.5508877219304826]\n",
      "staticData.csv split 3 scores: [0.5544551229841994, 0.5739731006906579, 0.5508877219304826]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5418853759765625\n",
      "     test_acc_epoch         0.8374524712562561\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.8806140422821045\n",
      "Number of features that impact on target 0: 4\n",
      "Explanation for target 0: (cns_medium & gender_male) | (gender_female & ~cns_high) | (gender_male & ~respiration_low & ~cns_high)\n",
      "Explanation accuracy: 0.4935281622332801\n",
      "Number of features that impact on target 1: 5\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [4/5]\n",
      "3368/842/1052\n",
      "[2933  435]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2933 2933]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.634\n",
      "Metric val_loss improved by 0.118 >= min_delta = 0.0. New best score: 0.516\n",
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.509\n",
      "Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 0.468\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.468. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5466947922582506, 0.5538390080374813, 0.5436271044432226]\n",
      "staticData.csv split 4 scores: [0.5466947922582506, 0.5538390080374813, 0.5436271044432226]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5633728504180908\n",
      "     test_acc_epoch         0.8241444826126099\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 1.7567577362060547\n",
      "Number of features that impact on target 0: 8\n",
      "Explanation for target 0: cns_medium | ~coagulation_high\n",
      "Explanation accuracy: 0.510337310354986\n",
      "Number of features that impact on target 1: 5\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [5/5]\n",
      "3368/842/1052\n",
      "[2931  437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2931 2931]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.645\n",
      "Metric val_loss improved by 0.111 >= min_delta = 0.0. New best score: 0.534\n",
      "Metric val_loss improved by 0.044 >= min_delta = 0.0. New best score: 0.490\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.483\n",
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.458\n",
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.441\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.441. Signaling Trainer to stop.\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5412742206577237, 0.541805805291549, 0.5407894736842105]\n",
      "staticData.csv split 5 scores: [0.5412742206577237, 0.541805805291549, 0.5407894736842105]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5548878908157349\n",
      "     test_acc_epoch          0.839353621006012\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 0.6849987506866455\n",
      "Number of features that impact on target 0: 7\n",
      "Explanation for target 0: (respiration_low & renal_low & cns_medium) | (los_high & respiration_low & renal_low & cns_low)\n",
      "Explanation accuracy: 0.1235575059184532\n",
      "Number of features that impact on target 1: 5\n",
      "Explanation for target 1: ~gender_female & ~gender_male\n",
      "Explanation accuracy: 0.4654471544715447\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "\n",
    "    if file in hiddenLayers:\n",
    "        layers = hiddenLayers[file]\n",
    "    else:\n",
    "        print(\"Set layers for \" + file)\n",
    "        layers = [[20], 0.01]\n",
    "\n",
    "    print(f\"Training {file}\\n\")\n",
    "\n",
    "    data = datasets[file]\n",
    "\n",
    "    if \"PatientID\" in data.columns:\n",
    "        data = data.drop(columns=[\"PatientID\"])\n",
    "\n",
    "\n",
    "    targetName = \"Mortality14Days\"\n",
    "\n",
    "    targetSeries = data[targetName]\n",
    "    print(data[targetName].value_counts())\n",
    "    data = data.drop(columns=[targetName])\n",
    "\n",
    "    n_concepts = data.shape[1]\n",
    "    print(\"There are \" + str(n_concepts) + \" concepts\")\n",
    "    n_classes = 2   \n",
    "\n",
    "    splitResults_list = []\n",
    "\n",
    "    \"\"\" The following lines were taken from the MIMIC example code by Pietro Barbiero\"\"\"\n",
    "    \n",
    "    dataTensor = torch.FloatTensor(data.to_numpy())\n",
    "    targetTensor = one_hot(torch.tensor(targetSeries.values).to(torch.long)).to(torch.float)   \n",
    "\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    x = dataTensor\n",
    "    y = targetTensor\n",
    "\n",
    "    for split, (trainval_index, test_index) in enumerate(skf.split(x.cpu().detach().numpy(),\n",
    "                                                                y.argmax(dim=1).cpu().detach().numpy())):\n",
    "        print(f'Split [{split + 1}/{n_splits}]')\n",
    "\n",
    "\n",
    "        x_trainval, x_test = torch.FloatTensor(x[trainval_index]), torch.FloatTensor(x[test_index])\n",
    "        y_trainval, y_test = torch.FloatTensor(y[trainval_index]), torch.FloatTensor(y[test_index])\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.2, random_state=42)\n",
    "        print(f'{len(y_train)}/{len(y_val)}/{len(y_test)}')\n",
    "\n",
    "\n",
    "        \"\"\" End of reference code \"\"\"\n",
    "\n",
    "        print(pd.Series(np.argmax(y_train.numpy(), axis=1)).value_counts().values)\n",
    "\n",
    "        # For oversampling... \n",
    "        clf = SMOTEN(random_state=0)\n",
    "\n",
    "        x_train, y_train = clf.fit_resample(x_train.numpy(), np.argmax(y_train.numpy(), axis=1))\n",
    "\n",
    "        x_train = torch.FloatTensor(x_train)\n",
    "        y_train = one_hot(torch.tensor(y_train).to(torch.long)).to(torch.float)\n",
    "\n",
    "        print(pd.Series(np.argmax(y_train.numpy(), axis=1)).value_counts().values)\n",
    "\n",
    "        batch_size = 64\n",
    "\n",
    "        train_data = TensorDataset(x_train, y_train)\n",
    "        train_loader = DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "        # For random sampling...\n",
    "        # class_count = pd.Series(targetSeries).value_counts()\n",
    "        # print(class_count)\n",
    "        # weights = 1. / torch.FloatTensor(class_count.values)\n",
    "        # print(weights)\n",
    "        # train_weights = np.array([weights[t] for t in torch.argmax(y_train, axis=1).numpy()]).astype(np.float64)\n",
    "        # sampler = WeightedRandomSampler(train_weights, train_size)\n",
    "        # train_data = TensorDataset(x_train, y_train)\n",
    "        # train_loader = DataLoader(train_data, batch_size=train_size, sampler=sampler)\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20, verbose=True, mode='min')\n",
    "\n",
    "        logger = TensorBoardLogger(\"./runs/splits/\", name=file)\n",
    "\n",
    "        \"\"\" The following lines were taken from the MIMIC example code by Pietro Barbiero\"\"\"\n",
    "\n",
    "        val_data = TensorDataset(x_val, y_val)\n",
    "        test_data = TensorDataset(x_test, y_test)\n",
    "        val_loader = DataLoader(val_data, batch_size = len(x_val))\n",
    "        test_loader = DataLoader(test_data, batch_size = len(x_test))\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(dirpath=base_dir, monitor='val_loss', mode='min', save_top_k=1)\n",
    "        \n",
    "        trainer = Trainer(max_epochs=100, gpus=1, auto_lr_find=True, deterministic=True,\n",
    "                        check_val_every_n_epoch=1, default_root_dir=base_dir,\n",
    "                        weights_save_path=base_dir, callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "                        logger=logger, enable_progress_bar=False, gradient_clip_val=0.5)\n",
    "\n",
    "        model = Explainer(n_concepts=n_concepts, n_classes=n_classes, l1=1e-3, lr=layers[1],\n",
    "                        explainer_hidden=layers[0], temperature=0.7)\n",
    "\n",
    "        # Training the model\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model.freeze()\n",
    "\n",
    "        \"\"\" End of reference code \"\"\"\n",
    "\n",
    "        # Precision, Recall, F1\n",
    "        y_pred = torch.argmax(model(x_test), axis=1)\n",
    "        y_test_argmax = torch.argmax(y_test, axis=1)\n",
    "\n",
    "        scores = [f1_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                recall_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                precision_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro')]\n",
    "\n",
    "        print(f\"Before loading best: {scores}\")\n",
    "\n",
    "        # Loading in the best weights from training\n",
    "        model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "\n",
    "        # Precision, Recall, F1\n",
    "        scores = [f1_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                recall_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                precision_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro')]\n",
    "\n",
    "        print(f\"{file} split {split+1} scores: {scores}\")\n",
    "\n",
    "        print(\"\\nTesting...\\n\")\n",
    "        # test_loader is giving a new batch of testing values, hence why the output here is different than above.\n",
    "        model_results = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "\n",
    "        print(\"\\nExplaining\\n\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        try:\n",
    "\n",
    "            results, f = explain_with_timeout(model, val_data=val_loader, train_data=train_loader, test_data=test_loader,\n",
    "                                        topk_expl=3,\n",
    "                                        concepts=data.columns)\n",
    "\n",
    "        except FunctionTimedOut:\n",
    "            print(\"Explanation timed out, skipping...\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        \"\"\" The following lines were taken from the MIMIC example code by Pietro Barbiero\"\"\"\n",
    "\n",
    "        print(f\"Explaining time: {end - start}\")\n",
    "        results['model_accuracy'] = model_results[0]['test_acc_epoch']\n",
    "        results['extraction_time'] = end - start\n",
    "\n",
    "        for j in range(n_classes):\n",
    "            n_used_concepts = sum(model.model[0].concept_mask[j] > 0.5)\n",
    "            print(f\"Number of features that impact on target {j}: {n_used_concepts}\")\n",
    "            print(f\"Explanation for target {j}: {f[j]['explanation']}\")\n",
    "            print(f\"Explanation accuracy: {f[j]['explanation_accuracy']}\")\n",
    "\n",
    "        \"\"\" End of reference code \"\"\"\n",
    "\n",
    "\n",
    "        splitResults = [results['model_accuracy'], results['extraction_time'], *scores, f]\n",
    "\n",
    "        splitResults_list.append(splitResults)\n",
    "\n",
    "\n",
    "    results_dict[file] = splitResults_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to remove explanation attempts that returned None.\n",
    "\n",
    "def removeNoneExplanations(explanations):\n",
    "\n",
    "    toRemove = []\n",
    "    for idx, expl in enumerate(explanations):\n",
    "        if expl['explanation'] == None:\n",
    "            toRemove.append(idx)\n",
    "    for i in sorted(toRemove, reverse=True):\n",
    "        del explanations[i]\n",
    "\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average0 = class0DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average1 = class1DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average0 = class0DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average1 = class1DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average0 = class0DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average1 = class1DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average0 = class0DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average1 = class1DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average0 = class0DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average1 = class1DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average0 = class0DF.mean().values\n",
      "C:\\Users\\benma\\AppData\\Local\\Temp/ipykernel_26956/1696261679.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  average1 = class1DF.mean().values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>expl_acc_0</th>\n",
       "      <th>expl_fidelity_0</th>\n",
       "      <th>expl_comp_0</th>\n",
       "      <th>expl_acc_1</th>\n",
       "      <th>expl_fidelity_1</th>\n",
       "      <th>expl_comp_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breastCancer.csv</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusteredData.csv</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.92</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusteredDataSepsis.csv</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expertLabelledData.csv</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metricExtractedData.csv</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.69</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staticData.csv</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.51</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model_acc    f1  recall  precision  expl_acc_0  \\\n",
       "file                                                                      \n",
       "breastCancer.csv              0.96  0.95    0.95       0.95        0.87   \n",
       "clusteredData.csv             0.94  0.47    0.46       0.49        0.47   \n",
       "clusteredDataSepsis.csv       0.88  0.56    0.58       0.55        0.48   \n",
       "expertLabelledData.csv        0.93  0.53    0.53       0.53        0.45   \n",
       "metricExtractedData.csv       0.94  0.55    0.55       0.55        0.46   \n",
       "staticData.csv                0.82  0.56    0.57       0.56        0.37   \n",
       "\n",
       "                         expl_fidelity_0  expl_comp_0  expl_acc_1  \\\n",
       "file                                                                \n",
       "breastCancer.csv                    0.90          2.6        0.86   \n",
       "clusteredData.csv                   0.84          2.4        0.50   \n",
       "clusteredDataSepsis.csv             0.84          2.6        0.40   \n",
       "expertLabelledData.csv              0.78          2.4        0.44   \n",
       "metricExtractedData.csv             0.79          2.8        0.40   \n",
       "staticData.csv                      0.51          5.0        0.47   \n",
       "\n",
       "                         expl_fidelity_1  expl_comp_1  \n",
       "file                                                   \n",
       "breastCancer.csv                    0.88         2.60  \n",
       "clusteredData.csv                   0.92         4.50  \n",
       "clusteredDataSepsis.csv             0.58         1.40  \n",
       "expertLabelledData.csv              0.71         3.75  \n",
       "metricExtractedData.csv             0.69         4.33  \n",
       "staticData.csv                      0.90         2.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kFoldMeans = []\n",
    "\n",
    "\n",
    "for x in results_dict:\n",
    "\n",
    "    cols = ['file', 'model_accuracy', 'extraction_time', 'f1', 'recall', 'precision']\n",
    "\n",
    "    # Fetching results\n",
    "    rows = []\n",
    "\n",
    "    class0Explanations = []\n",
    "    class1Explanations = []\n",
    "\n",
    "    for split in results_dict[x]:\n",
    "        row = [x]\n",
    "        \n",
    "        # Model accuracy results\n",
    "        row.extend(split[:5])\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        # Explanation accuracy results\n",
    "        class0Explanations.append(split[5][0])\n",
    "        class1Explanations.append(split[5][1])\n",
    "\n",
    "\n",
    "    class0Explanations = removeNoneExplanations(class0Explanations)\n",
    "\n",
    "    class1Explanations = removeNoneExplanations(class1Explanations)\n",
    "\n",
    "    class0DF = pd.DataFrame(class0Explanations)\n",
    "    class1DF = pd.DataFrame(class1Explanations)\n",
    "\n",
    "    average0 = class0DF.mean().values\n",
    "    average1 = class1DF.mean().values\n",
    "\n",
    "    # If the explanation attempt returned None fill with zeros\n",
    "    if len(class0Explanations) == 0:\n",
    "        average0 = [0]*4\n",
    "\n",
    "    if len(class1Explanations) == 0:\n",
    "        average1 = [0]*4\n",
    "\n",
    "    df = pd.DataFrame(columns=cols, data=rows)\n",
    "\n",
    "    df = df.set_index('file')\n",
    "\n",
    "    combinedCols = list(df.describe().columns)\n",
    "\n",
    "    row = [x]\n",
    "    row.extend(np.round(df.describe().loc['mean'].values, 2))\n",
    "\n",
    "    row.extend(list(average0)[1:])\n",
    "    row.extend(list(average1)[1:])\n",
    "\n",
    "    kFoldMeans.append(row)\n",
    "\n",
    "\n",
    "\n",
    "# Getting average, formatting into a dataframe\n",
    "\n",
    "kFoldMeansCols = list(df.describe().columns)\n",
    "\n",
    "combinedCols.insert(0, \"file\")\n",
    "\n",
    "\n",
    "for idx, d in enumerate(results_dict[list(results_dict.keys())[0]][0][5]):\n",
    "    combinedCols.extend([str(x) + \"_\" + str(idx) for x in list(d)[2:]])\n",
    "\n",
    "\n",
    "totalMeans = pd.DataFrame(columns=combinedCols, data=kFoldMeans)\n",
    "\n",
    "totalMeans = totalMeans.set_index('file')\n",
    "\n",
    "cols = totalMeans.columns\n",
    "\n",
    "cols = [c.replace(\"explanation\", \"expl\").replace(\"accuracy\", \"acc\").replace(\"complexity\", \"comp\") for c in cols]\n",
    "\n",
    "totalMeans.columns = cols\n",
    "\n",
    "totalMeans = totalMeans.round(2)\n",
    "\n",
    "totalMeans = totalMeans.drop(\"extraction_time\", axis=1)\n",
    "\n",
    "display(totalMeans)\n",
    "\n",
    "\n",
    "\n",
    "timeNow = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "totalMeans.to_csv(f\"./processingCache/totalMeans{timeNow}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c0da8f5b93ec0612bb8aed25290fce040bcd4618fccb82f778dd001cba2969b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
