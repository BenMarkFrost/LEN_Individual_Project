{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from torch_explain.models.explainer import Explainer\n",
    "from torch_explain.logic.metrics import formula_consistency\n",
    "# from load_datasets import load_mimic\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from torch.nn.functional import one_hot\n",
    "from func_timeout import func_set_timeout, func_timeout, FunctionTimedOut\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breastCancer.csv', 'clusteredData.csv', 'expertLabelledData.csv', 'metricExtractedData.csv', 'staticData.csv']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"./categorisedData/\")\n",
    "\n",
    "\n",
    "datasets = {file : pd.read_csv(\"./categorisedData/\" + file) for file in files}\n",
    "\n",
    "\n",
    "print(files)\n",
    "\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_set_timeout(90)\n",
    "def explain_with_timeout(model, val_data, train_data, test_data, topk_expl, concepts):\n",
    "\n",
    "    return model.explain_class(val_dataloaders=val_data, train_dataloaders=train_data, test_dataloaders=test_data, topk_explanations=topk_expl, concept_names=concepts, max_minterm_complexity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training breastCancer.csv\n",
      "\n",
      "0    458\n",
      "1    241\n",
      "Name: Mortality14Days, dtype: int64\n",
      "features: 89\n",
      "Split [1/5]\n",
      "447/112/140\n",
      "[298 149]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[298 298]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.943019943019943, 0.9320652173913043, 0.9574442579717094]\n",
      "breastCancer.csv split 1 scores: [0.983985358041638, 0.9791666666666667, 0.9893617021276595]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9839853644371033\n",
      "     test_acc_epoch         0.9857142567634583\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 19.386082649230957\n",
      "Number of features that impact on target 0: 17\n",
      "Explanation for target 0: ~Clump_Thickness_7 & ~Bland_Chromatin_10\n",
      "Explanation accuracy: 0.5812562313060817\n",
      "Number of features that impact on target 1: 17\n",
      "Explanation for target 1: ~Clump_Thickness_1 & ~Bland_Chromatin_1\n",
      "Explanation accuracy: 0.5736774193548386\n",
      "Split [2/5]\n",
      "447/112/140\n",
      "[292 155]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292 292]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.960565601937919, 0.9628623188405797, 0.9583987441130298]\n",
      "breastCancer.csv split 2 scores: [0.9609353200513421, 0.9678442028985508, 0.9551663361974003]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9609352946281433\n",
      "     test_acc_epoch         0.9642857313156128\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 25.213157176971436\n",
      "Number of features that impact on target 0: 82\n",
      "Explanation for target 0: None\n",
      "Explanation accuracy: 0\n",
      "Number of features that impact on target 1: 80\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [3/5]\n",
      "447/112/140\n",
      "[302 145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302 302]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.960565601937919, 0.9628623188405797, 0.9583987441130298]\n",
      "breastCancer.csv split 3 scores: [0.9609353200513421, 0.9678442028985508, 0.9551663361974003]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9609352946281433\n",
      "     test_acc_epoch         0.9642857313156128\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 20.49801254272461\n",
      "Number of features that impact on target 0: 85\n",
      "Explanation for target 0: None\n",
      "Explanation accuracy: 0\n",
      "Number of features that impact on target 1: 86\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [4/5]\n",
      "447/112/140\n",
      "[293 154]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293 293]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.9529042386185244, 0.9529042386185244, 0.9529042386185244]\n",
      "breastCancer.csv split 4 scores: [0.9765611920308053, 0.9788069073783359, 0.9744444444444444]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.9765611886978149\n",
      "     test_acc_epoch         0.9785714149475098\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n",
      "Explaining time: 21.208857536315918\n",
      "Number of features that impact on target 0: 85\n",
      "Explanation for target 0: None\n",
      "Explanation accuracy: 0\n",
      "Number of features that impact on target 1: 86\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [5/5]\n",
      "448/112/139\n",
      "[296 152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 3.6 K \n",
      "-------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296 296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.9357076780758558, 0.9314331501831502, 0.9405095839177187]\n",
      "breastCancer.csv split 5 scores: [0.9687640449438202, 0.9780219780219781, 0.9615384615384616]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch          0.968764066696167\n",
      "     test_acc_epoch          0.971222996711731\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 20.565938711166382\n",
      "Number of features that impact on target 0: 66\n",
      "Explanation for target 0: Bland_Chromatin_1 | ~Cell_Size_Uniformity_5\n",
      "Explanation accuracy: 0.4795779441797141\n",
      "Number of features that impact on target 1: 63\n",
      "Explanation for target 1: Cell_Size_Uniformity_6 | Marginal_Adhesion_7\n",
      "Explanation accuracy: 0.5642633228840126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clusteredData.csv\n",
      "\n",
      "0    924\n",
      "1     35\n",
      "Name: Mortality14Days, dtype: int64\n",
      "features: 50\n",
      "Split [1/5]\n",
      "613/154/192\n",
      "[587  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: ./runs/splits/clusteredData.csv\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.1 K \n",
      "-------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[587 587]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5552123552123552, 0.5552123552123552, 0.5552123552123552]\n",
      "clusteredData.csv split 1 scores: [0.488, 0.4945945945945946, 0.48157894736842105]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4880000054836273\n",
      "     test_acc_epoch              0.953125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 29.431500673294067\n",
      "Number of features that impact on target 0: 9\n",
      "Explanation for target 0: Hemoglobin_Mean_high | ~Hemoglobin_Mean_high\n",
      "Explanation accuracy: 0.4907161803713528\n",
      "Number of features that impact on target 1: 9\n",
      "Explanation for target 1: CVP_Mean_low & Platelets_Mean_high\n",
      "Explanation accuracy: 0.47967479674796754\n",
      "Split [2/5]\n",
      "613/154/192\n",
      "[591  22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.1 K \n",
      "-------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[591 591]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.47540983606557374, 0.4702702702702703, 0.48066298342541436]\n",
      "clusteredData.csv split 2 scores: [0.5606407322654462, 0.605019305019305, 0.5482954545454546]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5606407523155212\n",
      "     test_acc_epoch         0.9010416865348816\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 25.820793867111206\n",
      "Number of features that impact on target 0: 16\n",
      "Explanation for target 0: Arterial BP Mean_StdDev_high | ~Arterial BP Mean_StdDev_high | ~SVR_Mean_low\n",
      "Explanation accuracy: 0.4907161803713528\n",
      "Number of features that impact on target 1: 19\n",
      "Explanation for target 1: ~CVP_Mean_low & ~SVR_StdDev_high\n",
      "Explanation accuracy: 0.46114561766735684\n",
      "Split [3/5]\n",
      "613/154/192\n",
      "[592  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.1 K \n",
      "-------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592 592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.48663101604278075, 0.4918918918918919, 0.48148148148148145]\n",
      "clusteredData.csv split 3 scores: [0.473972602739726, 0.46756756756756757, 0.48055555555555557]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4739726483821869\n",
      "     test_acc_epoch         0.9010416865348816\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 24.676698446273804\n",
      "Number of features that impact on target 0: 15\n",
      "Explanation for target 0: Arterial pH_StdDev_low | ~Arterial pH_StdDev_low\n",
      "Explanation accuracy: 0.4907161803713528\n",
      "Number of features that impact on target 1: 13\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [4/5]\n",
      "613/154/192\n",
      "[589  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.1 K \n",
      "-------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589 589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.488, 0.4945945945945946, 0.48157894736842105]\n",
      "clusteredData.csv split 4 scores: [0.5293439077144917, 0.5416988416988417, 0.525]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5293439030647278\n",
      "     test_acc_epoch         0.9114583134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 24.87185263633728\n",
      "Number of features that impact on target 0: 6\n",
      "Explanation for target 0: Arterial BP Mean_Mean_low | Arterial PaCO2_Mean_high\n",
      "Explanation accuracy: 0.5524475524475525\n",
      "Number of features that impact on target 1: 3\n",
      "Explanation for target 1: ~Arterial BP [Diastolic]_Mean_high\n",
      "Explanation accuracy: 0.2827324478178368\n",
      "Split [5/5]\n",
      "614/154/191\n",
      "[589  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.1 K \n",
      "-------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589 589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.48793565683646106, 0.4945652173913043, 0.48148148148148145]\n",
      "clusteredData.csv split 5 scores: [0.48097826086956524, 0.48097826086956524, 0.48097826086956524]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.48097825050354004\n",
      "     test_acc_epoch          0.926701545715332\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 27.248028993606567\n",
      "Number of features that impact on target 0: 10\n",
      "Explanation for target 0: Arterial PaO2_Mean_high | Hemoglobin_Mean_high | ~Hemoglobin_Mean_high\n",
      "Explanation accuracy: 0.49066666666666664\n",
      "Number of features that impact on target 1: 4\n",
      "Explanation for target 1: ~Platelets_StdDev_low & ~Platelets_StdDev_medium\n",
      "Explanation accuracy: 0.47527472527472525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training expertLabelledData.csv\n",
      "\n",
      "0    1077\n",
      "1      49\n",
      "Name: Mortality14Days, dtype: int64\n",
      "features: 153\n",
      "Split [1/5]\n",
      "720/180/226\n",
      "[691  29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: ./runs/splits/expertLabelledData.csv\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 6.2 K \n",
      "-------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[691 691]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5330010939589158, 0.5314814814814814, 0.5348182283666154]\n",
      "expertLabelledData.csv split 1 scores: [0.5511014806789455, 0.5675925925925926, 0.543452380952381]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5511015057563782\n",
      "     test_acc_epoch         0.9026548862457275\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 25.081281661987305\n",
      "Number of features that impact on target 0: 147\n",
      "Explanation for target 0: CVP_Min_medium | ~CVP_Min_medium\n",
      "Explanation accuracy: 0.48868778280542985\n",
      "Number of features that impact on target 1: 149\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [2/5]\n",
      "720/181/225\n",
      "[688  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 6.2 K \n",
      "-------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688 688]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.48394495412844035, 0.48842592592592593, 0.47954545454545455]\n",
      "expertLabelledData.csv split 2 scores: [0.4827586206896552, 0.4861111111111111, 0.4794520547945205]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4827585816383362\n",
      "     test_acc_epoch         0.9333333373069763\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 26.344837427139282\n",
      "Number of features that impact on target 0: 12\n",
      "Explanation for target 0: ~AST_medium | ~INR_medium\n",
      "Explanation accuracy: 0.4874715261958998\n",
      "Number of features that impact on target 1: 20\n",
      "Explanation for target 1: CaO2_low & ~Temperature_C_Max_high\n",
      "Explanation accuracy: 0.29463970328505823\n",
      "Split [3/5]\n",
      "720/181/225\n",
      "[690  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 6.2 K \n",
      "-------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[690 690]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5468873128447597, 0.5651162790697675, 0.5395927601809954]\n",
      "expertLabelledData.csv split 3 scores: [0.5598591549295775, 0.5720930232558139, 0.5524712254570074]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5598591566085815\n",
      "     test_acc_epoch         0.9111111164093018\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 26.064717769622803\n",
      "Number of features that impact on target 0: 12\n",
      "Explanation for target 0: Hamoglobin_high | Hamoglobin_medium\n",
      "Explanation accuracy: 0.4899611398963731\n",
      "Number of features that impact on target 1: 8\n",
      "Explanation for target 1: CVP_Min_high & ~Arterial_PaCO2_medium\n",
      "Explanation accuracy: 0.5717644424540976\n",
      "Split [4/5]\n",
      "720/181/225\n",
      "[690  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 6.2 K \n",
      "-------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[690 690]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5415025132454829, 0.536046511627907, 0.5507863695937091]\n",
      "expertLabelledData.csv split 4 scores: [0.5290697674418605, 0.5290697674418605, 0.5290697674418605]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5290697813034058\n",
      "     test_acc_epoch         0.9200000166893005\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 24.244171142578125\n",
      "Number of features that impact on target 0: 25\n",
      "Explanation for target 0: Arterial_BP_Diastolic_low | Arterial_pH_Max_low\n",
      "Explanation accuracy: 0.5157124408092983\n",
      "Number of features that impact on target 1: 9\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [5/5]\n",
      "720/181/225\n",
      "[690  30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 6.2 K \n",
      "-------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[690 690]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5220900594732371, 0.5244186046511629, 0.5205399061032864]\n",
      "expertLabelledData.csv split 5 scores: [0.48630136986301364, 0.49534883720930234, 0.47757847533632286]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.48630136251449585\n",
      "     test_acc_epoch         0.9466667175292969\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 25.463199377059937\n",
      "Number of features that impact on target 0: 13\n",
      "Explanation for target 0: SVI_high | ~SVI_high\n",
      "Explanation accuracy: 0.48863636363636365\n",
      "Number of features that impact on target 1: 12\n",
      "Explanation for target 1: ~Hamoglobin_high\n",
      "Explanation accuracy: 0.3937552831783601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metricExtractedData.csv\n",
      "\n",
      "0    924\n",
      "1     35\n",
      "Name: Mortality14Days, dtype: int64\n",
      "features: 70\n",
      "Split [1/5]\n",
      "613/154/192\n",
      "[587  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: ./runs/splits/metricExtractedData.csv\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.9 K \n",
      "-------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[587 587]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5698924731182795, 0.5606177606177606, 0.5839572192513369]\n",
      "metricExtractedData.csv split 1 scores: [0.5434782608695652, 0.5498069498069498, 0.5391621129326047]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch          0.54347825050354\n",
      "     test_acc_epoch         0.9270833134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 26.66673469543457\n",
      "Number of features that impact on target 0: 10\n",
      "Explanation for target 0: CVP__root_mean_square_high | ~CVP__root_mean_square_high\n",
      "Explanation accuracy: 0.4907161803713528\n",
      "Number of features that impact on target 1: 8\n",
      "Explanation for target 1: CVP__quantile__q_0.1_very_low\n",
      "Explanation accuracy: 0.48369747899159665\n",
      "Split [2/5]\n",
      "613/154/192\n",
      "[591  22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.9 K \n",
      "-------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[591 591]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.48525469168900803, 0.4891891891891892, 0.48138297872340424]\n",
      "metricExtractedData.csv split 2 scores: [0.5919854280510017, 0.6185328185328185, 0.5770969362129583]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5919854640960693\n",
      "     test_acc_epoch         0.9270833134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 25.25193977355957\n",
      "Number of features that impact on target 0: 7\n",
      "Explanation for target 0: CVP__quantile__q_0.2_medium | CVP__quantile__q_0.7_very_low | ~CVP__quantile__q_0.7_very_low\n",
      "Explanation accuracy: 0.4907161803713528\n",
      "Number of features that impact on target 1: 12\n",
      "Explanation for target 1: ~CVP__quantile__q_0.2_high & ~CVP__variation_coefficient_low\n",
      "Explanation accuracy: 0.2952576714138893\n",
      "Split [3/5]\n",
      "613/154/192\n",
      "[592  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.9 K \n",
      "-------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592 592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.4810810810810811, 0.4810810810810811, 0.4810810810810811]\n",
      "metricExtractedData.csv split 3 scores: [0.48525469168900803, 0.4891891891891892, 0.48138297872340424]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4852546751499176\n",
      "     test_acc_epoch         0.9427083134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 25.822234630584717\n",
      "Number of features that impact on target 0: 10\n",
      "Explanation for target 0: CVP__minimum_low | ~CVP__c3__lag_1_low\n",
      "Explanation accuracy: 0.5060457936712117\n",
      "Number of features that impact on target 1: 10\n",
      "Explanation for target 1: CVP__c3__lag_1_low & ~CVP__minimum_very_low\n",
      "Explanation accuracy: 0.4498567335243553\n",
      "Split [4/5]\n",
      "613/154/192\n",
      "[589  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.9 K \n",
      "-------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589 589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.4696132596685083, 0.4594594594594595, 0.480225988700565]\n",
      "metricExtractedData.csv split 4 scores: [0.4696132596685083, 0.4594594594594595, 0.480225988700565]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.4696132242679596\n",
      "     test_acc_epoch         0.8854166865348816\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 25.723774194717407\n",
      "Number of features that impact on target 0: 15\n",
      "Explanation for target 0: CVP__quantile__q_0.2_high | CVP__minimum_high | ~CVP__quantile__q_0.2_high\n",
      "Explanation accuracy: 0.4907161803713528\n",
      "Number of features that impact on target 1: 9\n",
      "Explanation for target 1: ~CVP__quantile__q_0.3_high & ~CVP__quantile__q_0.7_very_low\n",
      "Explanation accuracy: 0.3530608840700584\n",
      "Split [5/5]\n",
      "614/154/191\n",
      "[589  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 2.9 K \n",
      "-------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[589 589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.4738292011019284, 0.4673913043478261, 0.48044692737430167]\n",
      "metricExtractedData.csv split 5 scores: [0.618346957311535, 0.6265527950310559, 0.6113387978142076]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.6183469891548157\n",
      "     test_acc_epoch         0.9424083828926086\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 24.375815629959106\n",
      "Number of features that impact on target 0: 16\n",
      "Explanation for target 0: ~CVP__minimum_medium\n",
      "Explanation accuracy: 0.5151951704740344\n",
      "Number of features that impact on target 1: 11\n",
      "Explanation for target 1: ~CVP__quantile__q_0.1_very_low & ~CVP__quantile__q_0.6_low\n",
      "Explanation accuracy: 0.18942694134509994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training staticData.csv\n",
      "\n",
      "0    4584\n",
      "1     678\n",
      "Name: Mortality14Days, dtype: int64\n",
      "features: 31\n",
      "Split [1/5]\n",
      "3367/842/1053\n",
      "[2928  439]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: ./runs/splits/staticData.csv\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2928 2928]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5608028930550484, 0.5912542497915196, 0.5578189300411522]\n",
      "staticData.csv split 1 scores: [0.5896532084434671, 0.6039515042658284, 0.5821379694865427]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5896531939506531\n",
      "     test_acc_epoch          0.790123462677002\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 52.41857981681824\n",
      "Number of features that impact on target 0: 8\n",
      "Explanation for target 0: cns_low | cns_medium\n",
      "Explanation accuracy: 0.527226826786394\n",
      "Number of features that impact on target 1: 12\n",
      "Explanation for target 1: sofa_high\n",
      "Explanation accuracy: 0.5636740331491712\n",
      "Split [2/5]\n",
      "3367/842/1053\n",
      "[2946  421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2946 2946]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5524891138048724, 0.5701616524472385, 0.549076795369394]\n",
      "staticData.csv split 2 scores: [0.5563984903768487, 0.5674514080441337, 0.5521635599211221]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5563985109329224\n",
      "     test_acc_epoch         0.7701804041862488\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 48.99111080169678\n",
      "Number of features that impact on target 0: 9\n",
      "Explanation for target 0: cns_low | cns_medium\n",
      "Explanation accuracy: 0.5299050857546529\n",
      "Number of features that impact on target 1: 7\n",
      "Explanation for target 1: age_high\n",
      "Explanation accuracy: 0.4692635861249618\n",
      "Split [3/5]\n",
      "3368/842/1052\n",
      "[2935  433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2935 2935]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.4979155525654516, 0.5842118017690536, 0.5389777910715621]\n",
      "staticData.csv split 3 scores: [0.5668642951251647, 0.5656286602851488, 0.5682346913922414]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5668643116950989\n",
      "     test_acc_epoch         0.8098859190940857\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 48.55330443382263\n",
      "Number of features that impact on target 0: 10\n",
      "Explanation for target 0: ~sofa_high\n",
      "Explanation accuracy: 0.5871578876146013\n",
      "Number of features that impact on target 1: 6\n",
      "Explanation for target 1: ~coagulation_medium & ~gender_female\n",
      "Explanation accuracy: 0.44951725113276\n",
      "Split [4/5]\n",
      "3368/842/1052\n",
      "[2933  435]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2933 2933]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5678403755868544, 0.6104446867805646, 0.5655582460262281]\n",
      "staticData.csv split 4 scores: [0.5946248463236957, 0.5937356112928631, 0.5955533596837945]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5946248173713684\n",
      "     test_acc_epoch         0.8203421831130981\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 50.615866899490356\n",
      "Number of features that impact on target 0: 23\n",
      "Explanation for target 0: None\n",
      "Explanation accuracy: 0\n",
      "Number of features that impact on target 1: 29\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n",
      "Split [5/5]\n",
      "3368/842/1052\n",
      "[2931  437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Users\\benma\\OneDrive\\Kings\\Modules\\Term 2\\Individual Project\\LEN Individual Project\\Notebooks\\runs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Sequential       | 1.3 K \n",
      "-------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2931 2931]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading best: [0.5389053186557233, 0.5576515540714102, 0.5383500200240289]\n",
      "staticData.csv split 5 scores: [0.5234960532911386, 0.5234234523503725, 0.5235712266246617]\n",
      "\n",
      "Testing...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      f1_test_epoch         0.5234960317611694\n",
      "     test_acc_epoch          0.786121666431427\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Explaining\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benma\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining time: 46.974565505981445\n",
      "Number of features that impact on target 0: 13\n",
      "Explanation for target 0: gender_male | ~coagulation_medium\n",
      "Explanation accuracy: 0.48539133738601825\n",
      "Number of features that impact on target 1: 13\n",
      "Explanation for target 1: None\n",
      "Explanation accuracy: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# print(os.listdir(\".\"))\n",
    "\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    # file = \"breastCancer.csv\"\n",
    "\n",
    "\n",
    "    print(f\"Training {file}\\n\")\n",
    "\n",
    "    data = datasets[file]\n",
    "\n",
    "    if \"PatientID\" in data.columns:\n",
    "        data = data.drop(columns=[\"PatientID\"])\n",
    "\n",
    "\n",
    "    targetName = \"Mortality14Days\"\n",
    "\n",
    "    targetSeries = data[targetName]\n",
    "    print(data[targetName].value_counts())\n",
    "    data = data.drop(columns=[targetName])\n",
    "\n",
    "    dataTensor = torch.FloatTensor(data.to_numpy())\n",
    "    targetTensor = one_hot(torch.tensor(targetSeries.values).to(torch.long)).to(torch.float)\n",
    "\n",
    "    # x, y, concept_names = load_mimic()\n",
    "    # base_dir=\"./LEN_test/pytorch_explain/experiments/elens/data\"\n",
    "\n",
    "\n",
    "    dataset = TensorDataset(dataTensor, targetTensor)\n",
    "\n",
    "    total_loader = DataLoader(dataset)\n",
    "\n",
    "\n",
    "    train_size = int(len(dataset) * 0.5)\n",
    "    val_size = (len(dataset) - train_size) // 2\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "    train_loader = DataLoader(train_data, batch_size=train_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=val_size)\n",
    "    test_loader = DataLoader(test_data, batch_size=test_size)\n",
    "    n_concepts = next(iter(train_loader))[0].shape[1]\n",
    "    n_classes = 2\n",
    "    # print(\"feature names: \", concept_names)\n",
    "    print(\"features:\", n_concepts)\n",
    "    # print(n_classes)\n",
    "\n",
    "    # %% md\n",
    "\n",
    "    ## 5-fold cross-validation with explainer network\n",
    "\n",
    "    # %%\n",
    "\n",
    "    seed_everything(42)\n",
    "\n",
    "    base_dir = f'./runs'\n",
    "    # os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    results_list = []\n",
    "    feature_selection = []\n",
    "    explanations = {i: [] for i in range(n_classes)}\n",
    "\n",
    "    explanations_list = []\n",
    "    splitResults_list = []\n",
    "    scores_list = []\n",
    "\n",
    "\n",
    "    x = dataTensor\n",
    "    y = targetTensor\n",
    "\n",
    "    for split, (trainval_index, test_index) in enumerate(skf.split(x.cpu().detach().numpy(),\n",
    "                                                                y.argmax(dim=1).cpu().detach().numpy())):\n",
    "        print(f'Split [{split + 1}/{n_splits}]')\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x_trainval, x_test = torch.FloatTensor(x[trainval_index]), torch.FloatTensor(x[test_index])\n",
    "        y_trainval, y_test = torch.FloatTensor(y[trainval_index]), torch.FloatTensor(y[test_index])\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.2, random_state=42)\n",
    "        print(f'{len(y_train)}/{len(y_val)}/{len(y_test)}')\n",
    "\n",
    "        print(pd.Series(np.argmax(y_train.numpy(), axis=1)).value_counts().values)\n",
    "\n",
    "        clf = SMOTEN(random_state=0)\n",
    "\n",
    "        x_train, y_train = clf.fit_resample(x_train.numpy(), np.argmax(y_train.numpy(), axis=1))\n",
    "\n",
    "        x_train = torch.FloatTensor(x_train)\n",
    "        y_train = one_hot(torch.tensor(y_train).to(torch.long)).to(torch.float)\n",
    "\n",
    "        print(pd.Series(np.argmax(y_train.numpy(), axis=1)).value_counts().values)\n",
    "\n",
    "\n",
    "        train_data = TensorDataset(x_train, y_train)\n",
    "        val_data = TensorDataset(x_val, y_val)\n",
    "        test_data = TensorDataset(x_test, y_test)\n",
    "        train_loader = DataLoader(train_data, batch_size=train_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=val_size)\n",
    "        test_loader = DataLoader(test_data, batch_size=test_size)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(dirpath=base_dir, monitor='f1_val', mode='max', save_top_k=1)\n",
    "\n",
    "        logger = TensorBoardLogger(\"./runs/splits/\", name=file)\n",
    "\n",
    "        trainer = Trainer(max_epochs=400, gpus=1, auto_lr_find=True, deterministic=True,\n",
    "                        check_val_every_n_epoch=1, default_root_dir=base_dir,\n",
    "                        weights_save_path=base_dir, callbacks=[checkpoint_callback],\n",
    "                        logger=logger, enable_progress_bar=False, gradient_clip_val=0.5)\n",
    "\n",
    "\n",
    "        model = Explainer(n_concepts=n_concepts, n_classes=n_classes, l1=1e-3, lr=0.01,\n",
    "                        explainer_hidden=[20], temperature=0.7)\n",
    "\n",
    "        start = time.time()\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        # print(f\"Gamma: {model.model[0].concept_mask}\")\n",
    "        model.freeze()\n",
    "\n",
    "        # Precision, Recall, F1\n",
    "        y_pred = torch.argmax(model(x_test), axis=1)\n",
    "        y_test_argmax = torch.argmax(y_test, axis=1)\n",
    "\n",
    "        scores = [f1_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                recall_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                precision_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro')]\n",
    "\n",
    "        print(f\"Before loading best: {scores}\")\n",
    "\n",
    "        # scores_list.append(scores)\n",
    "    \n",
    "        model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "\n",
    "        \n",
    "\n",
    "        # Precision, Recall, F1\n",
    "\n",
    "        scores = [f1_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                recall_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro'), \n",
    "                precision_score(y_test_argmax.numpy(), y_pred.numpy(), average='macro')]\n",
    "\n",
    "        print(f\"{file} split {split+1} scores: {scores}\")\n",
    "\n",
    "        print(\"\\nTesting...\\n\")\n",
    "        model_results = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "        scores_list.append(scores)\n",
    "\n",
    "\n",
    "        print(\"\\nExplaining\\n\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            results, f = explain_with_timeout(model, val_data=val_loader, train_data=train_loader, test_data=test_loader,\n",
    "                                        topk_expl=10,\n",
    "                                        concepts=data.columns)\n",
    "\n",
    "        except FunctionTimedOut:\n",
    "            print(\"Explanation timed out, skipping...\")\n",
    "            # explanations_list.append(None)\n",
    "            # results_list.append(None)\n",
    "            continue\n",
    "\n",
    "        end = time.time()\n",
    "        # explanations_list.append(f)\n",
    "\n",
    "        print(f\"Explaining time: {end - start}\")\n",
    "        results['model_accuracy'] = model_results[0]['test_acc_epoch']\n",
    "        results['extraction_time'] = end - start\n",
    "\n",
    "        results_list.append(results)\n",
    "        extracted_concepts = []\n",
    "        all_concepts = model.model[0].concept_mask[0] > 0.5\n",
    "        common_concepts = model.model[0].concept_mask[0] > 0.5\n",
    "        for j in range(n_classes):\n",
    "            # print(f[j]['explanation'])\n",
    "            n_used_concepts = sum(model.model[0].concept_mask[j] > 0.5)\n",
    "            print(f\"Number of features that impact on target {j}: {n_used_concepts}\")\n",
    "            print(f\"Explanation for target {j}: {f[j]['explanation']}\")\n",
    "            print(f\"Explanation accuracy: {f[j]['explanation_accuracy']}\")\n",
    "            explanations[j].append(f[j]['explanation'])\n",
    "            extracted_concepts.append(n_used_concepts)\n",
    "            all_concepts += model.model[0].concept_mask[j] > 0.5\n",
    "            common_concepts *= model.model[0].concept_mask[j] > 0.5\n",
    "\n",
    "        explanations_list.append(explanations)\n",
    "\n",
    "        results['extracted_concepts'] = np.mean(extracted_concepts)\n",
    "        results['common_concepts_ratio'] = sum(common_concepts) / sum(all_concepts)\n",
    "\n",
    "        \n",
    "\n",
    "        # prec_rec = precision_recall(y_pred, y_test_argmax, num_classes = n_classes)\n",
    "\n",
    "        # print(prec_rec)\n",
    "\n",
    "        # compare against standard feature selection\n",
    "        i_mutual_info = mutual_info_classif(x_trainval, y_trainval[:, 1])\n",
    "        i_chi2 = chi2(x_trainval, y_trainval[:, 1])[0]\n",
    "        i_chi2[np.isnan(i_chi2)] = 0\n",
    "        lasso = LassoCV(cv=5, random_state=0).fit(x_trainval, y_trainval[:, 1])\n",
    "        i_lasso = np.abs(lasso.coef_)\n",
    "        i_mu = model.model[0].concept_mask[1]\n",
    "        # print(model.model[0].concept_mask)\n",
    "        df = pd.DataFrame(np.hstack([\n",
    "            i_mu.numpy(),\n",
    "            # i_mutual_info / np.max(i_mutual_info),\n",
    "            # i_chi2 / np.max(i_chi2),\n",
    "            # i_lasso / np.max(i_lasso),\n",
    "        ]).T, columns=['feature importance'])\n",
    "        df['method'] = 'explainer'\n",
    "        # df.iloc[90:, 1] = 'MI'\n",
    "        # df.iloc[180:, 1] = 'CHI2'\n",
    "        # df.iloc[270:, 1] = 'Lasso'\n",
    "        df['feature'] = np.hstack([np.arange(0, n_concepts)])\n",
    "        feature_selection.append(df)\n",
    "\n",
    "        splitResults = [results['model_accuracy'], results['extraction_time'], *scores, f]\n",
    "\n",
    "        splitResults_list.append(splitResults)\n",
    "\n",
    "\n",
    "    results_dict[file] = splitResults_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train f1: 0.8449716428842396\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.argmax(model(x_train), axis=1)\n",
    "\n",
    "y = torch.argmax(y_train, axis=1)\n",
    "\n",
    "print(\"train f1:\" , f1_score(y, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1: 0.5234960532911386\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.argmax(model(x_test), axis=1)\n",
    "\n",
    "y = torch.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"test f1:\", f1_score(y, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_acc</th>\n",
       "      <th>extraction_time</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>expl_acc_0</th>\n",
       "      <th>expl_fidelity_0</th>\n",
       "      <th>expl_comp_0</th>\n",
       "      <th>expl_acc_1</th>\n",
       "      <th>expl_fidelity_1</th>\n",
       "      <th>expl_comp_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breastCancer.csv</th>\n",
       "      <td>0.97</td>\n",
       "      <td>21.37</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusteredData.csv</th>\n",
       "      <td>0.92</td>\n",
       "      <td>26.41</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expertLabelledData.csv</th>\n",
       "      <td>0.92</td>\n",
       "      <td>25.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metricExtractedData.csv</th>\n",
       "      <td>0.92</td>\n",
       "      <td>25.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staticData.csv</th>\n",
       "      <td>0.80</td>\n",
       "      <td>49.51</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model_acc  extraction_time    f1  recall  precision  \\\n",
       "file                                                                           \n",
       "breastCancer.csv              0.97            21.37  0.97    0.97       0.97   \n",
       "clusteredData.csv             0.92            26.41  0.51    0.52       0.50   \n",
       "expertLabelledData.csv        0.92            25.44  0.52    0.53       0.52   \n",
       "metricExtractedData.csv       0.92            25.57  0.54    0.55       0.54   \n",
       "staticData.csv                0.80            49.51  0.57    0.57       0.56   \n",
       "\n",
       "                         expl_acc_0  expl_fidelity_0  expl_comp_0  expl_acc_1  \\\n",
       "file                                                                            \n",
       "breastCancer.csv               0.21             0.28          0.8        0.23   \n",
       "clusteredData.csv              0.50             0.93          2.4        0.34   \n",
       "expertLabelledData.csv         0.49             0.89          2.0        0.25   \n",
       "metricExtractedData.csv        0.50             0.89          2.2        0.35   \n",
       "staticData.csv                 0.43             0.63          1.4        0.30   \n",
       "\n",
       "                         expl_fidelity_1  expl_comp_1  \n",
       "file                                                   \n",
       "breastCancer.csv                    0.25          0.8  \n",
       "clusteredData.csv                   0.58          1.4  \n",
       "expertLabelledData.csv              0.36          1.0  \n",
       "metricExtractedData.csv             0.56          1.8  \n",
       "staticData.csv                      0.41          0.8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best explanations on minority class:\n",
      "\n",
      "breastCancer.csv:\n",
      "\n",
      "[{'target_class': 0, 'explanation': '~Clump_Thickness_7 & ~Bland_Chromatin_10', 'explanation_accuracy': 0.5812562313060817, 'explanation_fidelity': 0.7357142857142858, 'explanation_complexity': 2}, {'target_class': 1, 'explanation': '~Clump_Thickness_1 & ~Bland_Chromatin_1', 'explanation_accuracy': 0.5736774193548386, 'explanation_fidelity': 0.5642857142857143, 'explanation_complexity': 2}]\n",
      "clusteredData.csv:\n",
      "\n",
      "[{'target_class': 0, 'explanation': 'Hemoglobin_Mean_high | ~Hemoglobin_Mean_high', 'explanation_accuracy': 0.4907161803713528, 'explanation_fidelity': 0.9895833333333334, 'explanation_complexity': 2}, {'target_class': 1, 'explanation': 'CVP_Mean_low & Platelets_Mean_high', 'explanation_accuracy': 0.47967479674796754, 'explanation_fidelity': 0.9479166666666666, 'explanation_complexity': 2}]\n",
      "expertLabelledData.csv:\n",
      "\n",
      "[{'target_class': 0, 'explanation': 'Hamoglobin_high | Hamoglobin_medium', 'explanation_accuracy': 0.4899611398963731, 'explanation_fidelity': 0.7777777777777778, 'explanation_complexity': 2}, {'target_class': 1, 'explanation': 'CVP_Min_high & ~Arterial_PaCO2_medium', 'explanation_accuracy': 0.5717644424540976, 'explanation_fidelity': 0.8666666666666667, 'explanation_complexity': 2}]\n",
      "metricExtractedData.csv:\n",
      "\n",
      "[{'target_class': 0, 'explanation': 'CVP__root_mean_square_high | ~CVP__root_mean_square_high', 'explanation_accuracy': 0.4907161803713528, 'explanation_fidelity': 0.953125, 'explanation_complexity': 2}, {'target_class': 1, 'explanation': 'CVP__quantile__q_0.1_very_low', 'explanation_accuracy': 0.48369747899159665, 'explanation_fidelity': 0.8229166666666666, 'explanation_complexity': 1}]\n",
      "staticData.csv:\n",
      "\n",
      "[{'target_class': 0, 'explanation': 'cns_low | cns_medium', 'explanation_accuracy': 0.527226826786394, 'explanation_fidelity': 0.7625830959164293, 'explanation_complexity': 2}, {'target_class': 1, 'explanation': 'sofa_high', 'explanation_accuracy': 0.5636740331491712, 'explanation_fidelity': 0.7996201329534662, 'explanation_complexity': 1}]\n"
     ]
    }
   ],
   "source": [
    "kFoldMeans = []\n",
    "\n",
    "bestExplanationsDict = {f:[0,0] for f in results_dict.keys()}\n",
    "\n",
    "# print(bestExplanationsDict)\n",
    "\n",
    "for x in results_dict:\n",
    "\n",
    "    cols = ['file', 'model_accuracy', 'extraction_time', 'f1', 'recall', 'precision']\n",
    "\n",
    "    # cols.extend(['model_accuracy', 'extraction_time', 'f1', 'recall', 'precision'])\n",
    "\n",
    "    for idx, d in enumerate(results_dict[x][0][5]):\n",
    "        cols.extend([str(x) + \"_\" + str(idx) for x in list(d)[1:]])\n",
    "\n",
    "    # print(cols)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for split in results_dict[x]:\n",
    "\n",
    "\n",
    "        if split[5][1]['explanation_accuracy'] > bestExplanationsDict[x][0]:\n",
    "            bestExplanationsDict[x] = [split[2], split[5]]\n",
    "\n",
    "        row = [x]\n",
    "\n",
    "        row.extend(split[:5])\n",
    "\n",
    "\n",
    "        for d in split[5]:\n",
    "\n",
    "            row.extend(list(d.values())[1:])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns=cols, data=rows)\n",
    "\n",
    "    df = df.set_index('file')\n",
    "\n",
    "    combinedCols = list(df.describe().columns)\n",
    "\n",
    "    # print(combinedCols)\n",
    "\n",
    "    row = [x]\n",
    "    row.extend(np.round(df.describe().loc['mean'].values, 2))\n",
    "\n",
    "    kFoldMeans.append(row)\n",
    "\n",
    "# print(kFoldMeans)\n",
    "\n",
    "\n",
    "\n",
    "kFoldMeansCols = list(df.describe().columns)\n",
    "\n",
    "combinedCols.insert(0, \"file\")\n",
    "\n",
    "\n",
    "# print(kFoldMeansCols)\n",
    "\n",
    "totalMeans = pd.DataFrame(columns=combinedCols, data=kFoldMeans)\n",
    "\n",
    "totalMeans = totalMeans.set_index('file')\n",
    "\n",
    "cols = totalMeans.columns\n",
    "\n",
    "cols = [c.replace(\"explanation\", \"expl\").replace(\"accuracy\", \"acc\").replace(\"complexity\", \"comp\") for c in cols]\n",
    "\n",
    "totalMeans.columns = cols\n",
    "\n",
    "display(totalMeans)\n",
    "\n",
    "print(\"Best explanations on minority class:\\n\")\n",
    "for i in bestExplanationsDict:\n",
    "    print(f\"{i}:\\n\")\n",
    "    print(bestExplanationsDict[i][1])\n",
    "\n",
    "\n",
    "\n",
    "totalMeans.to_csv(f\"./processingCache/totalMeans{date.today()}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c0da8f5b93ec0612bb8aed25290fce040bcd4618fccb82f778dd001cba2969b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
